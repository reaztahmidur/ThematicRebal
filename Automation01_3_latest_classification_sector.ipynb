{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd30684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique Perm ID: 11157\n",
      "['Syntax Thematic Cybersecurity Index', 'Syntax Thematic Cloud Compute Index', 'Syntax Thematic IoT Index', 'Syntax Thematic E-commerce Index', 'Syntax Thematic Battery Index', 'Syntax Thematic Bio Revolution Index', 'Syntax Thematic Clean Energy Index', 'Syntax Thematic Software-as-a-Service (SaaS) Index', 'Syntax Thematic Defensive Index', 'Syntax Thematic Infrastructure Index', 'Syntax Thematic Real Asset Index', 'Syntax Thematic Inflation Index', 'Syntax Thematic Digital Health Index']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import static_pulls as s_pulls \n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "\"\"\"import \"static_pulls\" and then call the method \n",
    "\"static_pulls.static_request(#today's date#, #list of unique ISINs from column AC#, ['OPID'], \n",
    "datatype = #placeholder#, write_df = False)\"\n",
    "setting write_df to false returns a pandas dataframe to the variable rather than writing a csv file to a folder\n",
    "\"\"\"\n",
    "\n",
    "def amendments(amend_df, input_df, max_year):\n",
    "    for index, row in amend_df.iterrows():\n",
    "        last_yr = row['last year']\n",
    "        index_name = row['Taxonomy']\n",
    "        isin = row['Identifier']\n",
    "        if last_yr == 'nan':\n",
    "            input_df.loc[input_df['ISIN'] == isin, index_name] = 0\n",
    "        else:\n",
    "            year_ls = list(range(int(float(last_yr) + 2), max_year + 1))\n",
    "            input_df.loc[(input_df['Snapshot Date (Year)'].isin(year_ls)) & (input_df['ISIN'] == isin), index_name] = 0\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def create_exp_table(df_new, row_labels):\n",
    "    \"\"\"Create the empty time series table\"\"\"\n",
    "    unique_perid = sorted(df_new['Perm ID'].unique())\n",
    "    columns = ['Row Labels'] + unique_perid\n",
    "    exp_table = pd.DataFrame(columns=columns)\n",
    "    exp_table['Row Labels'] = pd.Series(row_labels)\n",
    "    exp_table.set_index('Row Labels', inplace=True)   \n",
    "    return exp_table\n",
    "\n",
    "def create_timeseries(df):\n",
    "    \"\"\"Create the final time series by removing the last row and adding Year Used column\"\"\"\n",
    "    df_cleaned = df.iloc[:df.shape[0] - 1, :]\n",
    "    year_used = df_cleaned.index.astype(int) + 1\n",
    "    year_used = year_used.tolist()\n",
    "    df_op = df_cleaned.copy()\n",
    "    df_op.insert(loc=0, column='Year Used', value=year_used)\n",
    "    return df_op\n",
    "\n",
    "def exp_or_code(choice):\n",
    "    \"\"\"This method allows users to choose between max exposure or its sector code\"\"\"\n",
    "    if choice == 1:\n",
    "        return 'Max Weight'\n",
    "    else:\n",
    "        return 'Max Weighted Sector Code'\n",
    "\n",
    "def fill_NA_opid(isin_opid_map):\n",
    "    \"\"\"This method aims to fill NaN value of Perm ID by its corresponding ISIN\"\"\"\n",
    "    temp = isin_opid_map.copy()\n",
    "    for key, val in temp.items():\n",
    "        if val == 'NA':\n",
    "            temp[key] = key\n",
    "    return temp\n",
    "    \n",
    "def fill_exposure(df_tax_1, PG_bio_revo, exp_code):\n",
    "    \"\"\"Fill max exposure or the sector code into the time series table\"\"\"\n",
    "    for index, row in df_tax_1.iterrows():\n",
    "        year = row['fiscal_year']\n",
    "        perid = row['Perm ID']\n",
    "        PG_bio_revo.loc[year, perid] = row[exp_code]\n",
    "    return PG_bio_revo\n",
    "\n",
    "def front_back_fill(df):\n",
    "    \"\"\"Front fill and back fill the time series of each thematic index\"\"\"\n",
    "    output = df.ffill(axis=0)\n",
    "    output.fillna(method='bfill', inplace=True)\n",
    "    output.fillna(0, inplace=True)\n",
    "    return output\n",
    "\n",
    "def get_rbr_id(df_rbr, taxonomy_ls, level_ls):\n",
    "    \"\"\"This method aims to get the rbr id of the selected taxonomy and levels.\n",
    "    The input include a list of desired taxonomy and a list of desired levels.\n",
    "    taxonomy_ls is a list of strings, and level_ls is a list of integers.\"\"\"\n",
    "    sub_df = df_rbr.loc[(df_rbr.taxonomy.isin(taxonomy_ls)) & df_rbr.level.isin(level_ls)]\n",
    "    return sub_df.rbr_id.values\n",
    "\n",
    "def get_rbr_id_map(df_rbr, taxonomy_ls, level_ls):\n",
    "    \"\"\"Obtain the dictionary of key as rbr id and value as rbr name\"\"\"\n",
    "    sub_df = df_rbr.loc[(df_rbr.taxonomy.isin(taxonomy_ls)) & df_rbr.level.isin(level_ls)]\n",
    "    rbr_id_map = dict(zip(sub_df.rbr_id.apply(str), sub_df.name))\n",
    "    return rbr_id_map\n",
    "\n",
    "def get_rbr_data(df_rbr_data, rbr_id):\n",
    "    \"\"\"This method aims to get columns correspondent to the selected rbr id, and then clean \n",
    "    the data by taking the absolute value and replace 0 by 100000.\n",
    "    The rbr_id is a numpy array of integers. The df_rbr_data is the rbr data package.\"\"\"\n",
    "    output = []\n",
    "    for x in rbr_id:\n",
    "        col = str(x)\n",
    "        if col in df2.columns.tolist():\n",
    "            output.append(df2[col])\n",
    "        else:\n",
    "            print(col, ' -- ID Not Found in rbr data')\n",
    "    new_df = pd.concat(output, axis=1).abs()\n",
    "    new_df.replace(0, 100000, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def get_index_weight(df5_key, df_timeseries):\n",
    "    \"\"\"This method aims to obtain the columns of the time series table\"\"\"\n",
    "    output = [np.nan] * len(df5_key)\n",
    "    opid_list = df_timeseries.columns[1:]\n",
    "    for index, row in df5_key.iterrows():\n",
    "        year = row['Snapshot Date (Year)']\n",
    "        opid = str(row['Final ID'])\n",
    "        if opid in opid_list:\n",
    "            val = df_timeseries[df_timeseries['Year Used'] == year].loc[:,opid].values[0]\n",
    "            output[index] = val\n",
    "    return output\n",
    "\n",
    "def get_max_weight_code(get_rbr_data_op, rbr_id_map):\n",
    "    \"\"\"This method aims to obtain the column with max weight code.\"\"\"\n",
    "    max_weight = pd.DataFrame(get_rbr_data_op.max(axis=1), columns=['Max Weight'])\n",
    "    max_weight_code = pd.DataFrame(get_rbr_data_op.idxmax(axis='columns'), columns=['Max Weighted Sector Code'])\n",
    "    new_df = pd.concat([get_rbr_data_op, max_weight_code, max_weight], axis=1)\n",
    "    new_df['Max Weighted Sector Name'] = new_df['Max Weighted Sector Code'].map(rbr_id_map)\n",
    "    return new_df\n",
    "\n",
    "def get_row_label(df_tax_1):\n",
    "    \"\"\"Obtain the index of the time series table\"\"\"\n",
    "    row_labels = sorted(df_tax_1[df_tax_1['fiscal_year'].notna()]['fiscal_year'].unique().astype(int))\n",
    "    row_labels += [np.nan]\n",
    "    return row_labels\n",
    "\n",
    "def max_weight_code_name(index_list, level_1, df_fix, df1, df2, df3, df5, choice):\n",
    "    \"\"\"Obtain the dataframe ready for screening and weighting\"\"\"\n",
    "    # level_1 is a boolean variable which determines whether we focus on level 1 or the level with highest exp\n",
    "    output = df5.copy()\n",
    "    df5_key = df5[['Snapshot Date (Year)', 'Final ID']]\n",
    "    \n",
    "    for i in range(len(index_list)):\n",
    "        index_name = index_list[i]\n",
    "        tax_l = [index_name]\n",
    "        if level_1:\n",
    "            level_l = [1] #####\n",
    "        else:\n",
    "            level_l = df3[df3['Taxonomy Name Matched to Map'] == index_name]['Taxonomy Level Used'].values.tolist()\n",
    "            print(level_l)\n",
    "        print(tax_l)\n",
    "        rbr_id = get_rbr_id(df1, tax_l, level_l)\n",
    "        print(rbr_id)\n",
    "        rbr_id_map = get_rbr_id_map(df1, tax_l, level_l)\n",
    "        print(rbr_id_map)\n",
    "        \n",
    "        ############\n",
    "        \"\"\"\n",
    "        if index_name == 'Syntax Thematic Digital Health Index':\n",
    "            rbr_id = rbr_id[rbr_id != 46059]\n",
    "            rbr_id_map.pop(str(46059))\n",
    "            \n",
    "            print(rbr_id)\n",
    "            print(rbr_id_map)\"\"\"\n",
    "        ############\n",
    "        \n",
    "        rbr_data = get_rbr_data(df2, rbr_id)\n",
    "\n",
    "        max_weight_code = get_max_weight_code(rbr_data, rbr_id_map)\n",
    "        \n",
    "        df_new = pd.concat([df_fix, max_weight_code], axis=1)\n",
    "        \n",
    "        exp_code = exp_or_code(choice)\n",
    "        \n",
    "        sub_df = df_new[df_new[exp_code].notna()][['Perm ID', 'fiscal_year', exp_code]] # extract non-NaN\n",
    "        row_labels = get_row_label(df_new)\n",
    "        exp_table = create_exp_table(df_new, row_labels)\n",
    "        \n",
    "        exp_filled = fill_exposure(df_new, exp_table, exp_code) # fill exposure or code table\n",
    "        exp_filled_fb = front_back_fill(exp_filled) # front and back fill\n",
    "        exp_timeseries = create_timeseries(exp_filled_fb)\n",
    "\n",
    "        opid_list = exp_timeseries.columns[1:]\n",
    "        col_data = get_index_weight(df5_key, exp_timeseries)\n",
    "        #print(col_data[:10])\n",
    "        col_name = index_abv_list[i]\n",
    "        output.loc[:, col_name] = col_data # add a new column with column name equal to the index name\n",
    "        \n",
    "    return output   \n",
    "\n",
    "# change gvkey column from number to string\n",
    "#df2 = pd.read_excel('C:\\\\Users\\\\rzhou\\\\Downloads\\\\test.xlsx')\n",
    "df2 = pd.read_excel('no_filter_rbr_data_package_2023-06-13_v3.xlsx', converters={'gvkey': str})\n",
    "df1 = pd.read_excel('rbr_id_map_2023-06-12_no_filter.xlsx')\n",
    "df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "df3 = pd.read_excel('taxonomy_name_level4.xlsx')\n",
    "df5 = pd.read_excel('thematic_initial7_snapshot.xlsx')\n",
    "amend_df = pd.read_excel('classification_amend.xlsx')\n",
    "saas = pd.read_excel('SaaS.xlsx')\n",
    "df5['OPID'] = df5['OPID'].astype(str)\n",
    "df5['Final ID'] = df5['Final ID'].astype(str)\n",
    "df5['DSCD'] = df5['DSCD'].astype(str)\n",
    "df5['ISIN'] = df5['ISIN'].astype(str)\n",
    "\n",
    "# Front Fill gvkey due to the excel format\n",
    "df2['gvkey'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Front Fill fiscal_year after groupby gvkey\n",
    "# df2.update(df2.groupby('gvkey')['fiscal_year'].apply(lambda x: x.ffill()))\n",
    "df2['fiscal_year'] = df2.groupby('gvkey')['fiscal_year'].transform(lambda x: x.ffill())\n",
    "\n",
    "# Sort df2 by gvkey and fiscal_date\n",
    "df2 = df2.sort_values(['gvkey', 'fiscal_date'])\n",
    "\n",
    "# Drop duplicates of gvkey and fiscal_year and keep the last one\n",
    "df2.drop_duplicates(subset=['gvkey', 'fiscal_year'], keep='last', inplace=True)\n",
    "df2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# fill NAN in fiscal_year by latest year\n",
    "df2['fiscal_year'].fillna(2022, inplace=True)\n",
    "\n",
    "# create df_fix by extracting the first 4 columns\n",
    "df_fix = df2.iloc[:, :4]\n",
    "\n",
    "# Fill the empty ISIN by its corresponding gvkey\n",
    "df_fix['isin'].fillna(df_fix['gvkey'], inplace=True)\n",
    "\n",
    "# Add Perm ID based on ISIN\n",
    "ticks = df_fix['isin'].unique()\n",
    "asofdate = dt.date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Run static_request to pull OPID\n",
    "\"\"\"USE mappings from thematic_initial\"\"\"\n",
    "#get_stats = s_pulls.static_request(asofdate, ticks, ['OPID'], 'placeholder', write_df = False)\n",
    "#isin_opid_map = dict(zip(get_stats.Instrument, get_stats.Value))\n",
    "isin_opid_map = dict(zip(df5['ISIN'], df5['Final ID']))\n",
    "\n",
    "# Fill NaN Perm ID\n",
    "isin_opid_map2 = fill_NA_opid(isin_opid_map)\n",
    "df_fix.insert(0, 'Perm ID', df_fix['isin'].map(isin_opid_map2))\n",
    "df_fix['Perm ID'].fillna(df_fix['isin'], inplace=True)\n",
    "\n",
    "print('The number of unique Perm ID:', df_fix['Perm ID'].nunique())\n",
    "\n",
    "index_list = df3['Taxonomy Name Matched to Map'].values.tolist()\n",
    "print(index_list)\n",
    "\n",
    "index_abv_list = ['Cybersecurity', 'Cloud', 'IoT', 'E-Commerce', 'Battery', 'Bio Revolution', 'Clean Energy', \n",
    "                  'SaaS', 'Defensive', 'Infrastructure', 'Real Asset', 'Inflation','Digital Health']\n",
    "\n",
    "def max_weight_code_name(index_list, level_1, df_fix, df1, df2, df3, df5, choice):\n",
    "    \"\"\"Obtain the dataframe ready for screening and weighting\"\"\"\n",
    "    # level_1 is a boolean variable which determines whether we focus on level 1 or the level with highest exp\n",
    "    output = df5.copy()\n",
    "    df5_key = df5[['Snapshot Date (Year)', 'Final ID']]\n",
    "    \n",
    "    for i in range(len(index_list)):\n",
    "        index_name = index_list[i]\n",
    "        tax_l = [index_name]\n",
    "        if level_1:\n",
    "            level_l = [1]\n",
    "        else:\n",
    "            level_l = df3[df3['Taxonomy Name Matched to Map'] == index_name]['Taxonomy Level Used'].values.tolist()\n",
    "            print(level_l)\n",
    "        print(tax_l)\n",
    "        rbr_id = get_rbr_id(df1, tax_l, level_l)\n",
    "        print(rbr_id)\n",
    "        rbr_id_map = get_rbr_id_map(df1, tax_l, level_l)\n",
    "        print(rbr_id_map)\n",
    "        \n",
    "        ############\n",
    "        \"\"\"\n",
    "        if index_name == 'Syntax Thematic Digital Health Index':\n",
    "            rbr_id = rbr_id[rbr_id != 46059]\n",
    "            rbr_id_map.pop(str(46059))\n",
    "            \n",
    "            print(rbr_id)\n",
    "            print(rbr_id_map)\"\"\"\n",
    "        ############\n",
    "        \n",
    "        rbr_data = get_rbr_data(df2, rbr_id)\n",
    "\n",
    "        max_weight_code = get_max_weight_code(rbr_data, rbr_id_map)\n",
    "        \n",
    "        df_new = pd.concat([df_fix, max_weight_code], axis=1)\n",
    "        \n",
    "        exp_code = exp_or_code(choice)\n",
    "        \n",
    "        sub_df = df_new[df_new[exp_code].notna()][['Perm ID', 'fiscal_year', exp_code]] # extract non-NaN\n",
    "        row_labels = get_row_label(df_new)\n",
    "        exp_table = create_exp_table(df_new, row_labels)\n",
    "        \n",
    "        exp_filled = fill_exposure(df_new, exp_table, exp_code) # fill exposure or code table\n",
    "        exp_filled_fb = front_back_fill(exp_filled) # front and back fill\n",
    "        exp_timeseries = create_timeseries(exp_filled_fb)\n",
    "\n",
    "        opid_list = exp_timeseries.columns[1:]\n",
    "        col_data = get_index_weight(df5_key, exp_timeseries)\n",
    "        #print(col_data[:10])\n",
    "        col_name = index_abv_list[i]\n",
    "        output.loc[:, col_name] = col_data # add a new column with column name equal to the index name\n",
    "        \n",
    "    return output   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8bf73db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "['Syntax Thematic Cybersecurity Index']\n",
      "[44240 44246 44243]\n",
      "{'44240': 'Cybersecurity Software', '44246': 'Cybersecurity Services', '44243': 'Cybersecurity Hardware'}\n",
      "[2]\n",
      "['Syntax Thematic Cloud Compute Index']\n",
      "[46082 46081 46085 46084 46083 46086]\n",
      "{'46082': 'Cloud Compute Hardware', '46081': 'Cloud Services', '46085': 'Content Delivery Network', '46084': 'Data Center & Colocation', '46083': 'Cloud Software', '46086': 'Database Software'}\n",
      "[2]\n",
      "['Syntax Thematic IoT Index']\n",
      "[46095 46096 46094 46090 46089 46093 46088 46097]\n",
      "{'46095': 'Telecommunications', '46096': 'Consumer IoT', '46094': 'Industrial', '46090': 'Utilities', '46089': 'Semiconductors', '46093': 'Smart Home', '46088': 'Smart Buildings', '46097': 'Automotive'}\n",
      "[3]\n",
      "['Syntax Thematic E-commerce Index']\n",
      "[46104 46100 46105 46103 46107 46101 46108 46110]\n",
      "{'46104': 'Shipping & Transportation', '46100': 'Online Marketplace', '46105': 'Warehousing REITs', '46103': 'Logistics Equipment & Services', '46107': 'Transaction Processing', '46101': 'Online Retail & Rental', '46108': 'Payment Software', '46110': 'E-Commerce Enabling Platforms'}\n",
      "[2]\n",
      "['Syntax Thematic Battery Index']\n",
      "[41764 41760 41762 41761 41765 41763]\n",
      "{'41764': 'Fuel Cell ', '41760': 'Batteries', '41762': 'Diversified Electronic Materials', '41761': 'Battery Materials', '41765': 'Charging Stations', '41763': 'Clean Energy Vehicles'}\n",
      "[2]\n",
      "['Syntax Thematic Bio Revolution Index']\n",
      "[44216 44214 44213 44210 44215]\n",
      "{'44216': 'Agriculture', '44214': 'Biomaterials', '44213': 'Food', '44210': 'Biofuels', '44215': 'Life Science'}\n",
      "[2]\n",
      "['Syntax Thematic Clean Energy Index']\n",
      "[44238 44218 44231 44237 44234 44224 44228]\n",
      "{'44238': 'Renewable Energy Financing', '44218': 'Renewable Power Generation', '44231': 'Electric Vehicles', '44237': 'Hydrogen & Fuel Cell', '44234': 'Biofuel & Ethanol', '44224': 'Renewable Power Equipment', '44228': 'Batteries and Electronic Materials'}\n",
      "[2]\n",
      "['Syntax Thematic Software-as-a-Service (SaaS) Index']\n",
      "[46122 46120 46117 46119 46124 46115 46116 46114 46121 46123 46113 46118]\n",
      "{'46122': 'Content Creation SaaS', '46120': 'Communications SaaS', '46117': 'Other Vertical Specific SaaS', '46119': 'Government SaaS', '46124': 'Diversified SaaS', '46115': 'Financial Technology SaaS', '46116': 'Sales and Marketing SaaS', '46114': 'Business Processing SaaS', '46121': 'IT SaaS', '46123': 'Supply Chain, Logistics, and Telematics SaaS', '46113': 'Healthcare & Life Science SaaS', '46118': 'Consumer SaaS'}\n",
      "[2]\n",
      "['Syntax Thematic Defensive Index']\n",
      "[46126 46129 46128 46127]\n",
      "{'46126': 'Defensive Utilities', '46129': 'Other Defensive Businesses', '46128': 'Defensive Healthcare', '46127': 'Consumer Staples and Necessities'}\n",
      "[2]\n",
      "['Syntax Thematic Infrastructure Index']\n",
      "[46139 46131 46140]\n",
      "{'46139': 'Construction and Engineering', '46131': 'Infrastructure Suppliers', '46140': 'Infrastructure Operators'}\n",
      "[3]\n",
      "['Syntax Thematic Real Asset Index']\n",
      "[46156 46210 46196 46165 46160 46178 46146]\n",
      "{'46156': 'Consumer Real Estate', '46210': 'Metal Commodities', '46196': 'Energy Commodities', '46165': 'Energy Infrastructure', '46160': 'Other Real Estate Rental', '46178': 'Transportation & Cell Tower', '46146': 'Commercial Real Estate'}\n",
      "[2]\n",
      "['Syntax Thematic Inflation Index']\n",
      "[46018 46027 46032 46019 46023]\n",
      "{'46018': 'Royalties', '46027': 'Infrastructure', '46032': 'Real Estate', '46019': 'Commodities', '46023': 'Clean Energy'}\n",
      "[2]\n",
      "['Syntax Thematic Digital Health Index']\n",
      "[46052 46063 46057 46054 46061]\n",
      "{'46052': 'Telehealth', '46063': 'Digital Healthcare Applications', '46057': 'Surgical Robot', '46054': 'Consumer Take Home/Digital Fitness', '46061': 'Online Pharmacies'}\n",
      "70.73686838150024\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# False means not limit to level 1\n",
    "# df_fix is the first 4 columns of rbr data with perm ID \n",
    "# df1 is the rbr map, df2 is the rbr data, and df5 is copied from the rbr selected calc (syntax 3000)\n",
    "#level1_weight = max_weight_code_name(index_list, True, df_fix, df1, df2, df3, df5, 1)\n",
    "level23_code = max_weight_code_name(index_list, False, df_fix, df1, df2, df3, df5, 2)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f9c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Real Asset\n",
       "0.0      2433\n",
       "46196     190\n",
       "46146      88\n",
       "46210      86\n",
       "46165      71\n",
       "46160      65\n",
       "46178      22\n",
       "46156      20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level23_code['Real Asset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e2c0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique Perm ID: 11157\n",
      "The number of unique Perm ID: 2997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2975"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The number of unique Perm ID:', df_fix['Perm ID'].nunique())\n",
    "print('The number of unique Perm ID:', df5['Final ID'].nunique())\n",
    "\n",
    "a = df_fix['isin'].unique().tolist()\n",
    "b = df5['ISIN'].unique().tolist()\n",
    "c = []\n",
    "for i in b:\n",
    "    if str(i) not in a:\n",
    "        c.append(i)\n",
    "        \n",
    "level1_exp = level23_code[~level23_code['ISIN'].isin(c)]\n",
    "#level1_exp = level1_weight[~level23_code['ISIN'].isin(c)]\n",
    "level1_exp_copy = level1_exp.copy()\n",
    "len(level1_exp_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7823a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Snapshot Date</th>\n",
       "      <th>Snapshot Date (Year)</th>\n",
       "      <th>Weight Date</th>\n",
       "      <th>Rebal Date</th>\n",
       "      <th>Ex Date</th>\n",
       "      <th>SN3k index weight</th>\n",
       "      <th>OPID</th>\n",
       "      <th>DSCD</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Final ID</th>\n",
       "      <th>...</th>\n",
       "      <th>E-Commerce</th>\n",
       "      <th>Battery</th>\n",
       "      <th>Bio Revolution</th>\n",
       "      <th>Clean Energy</th>\n",
       "      <th>SaaS</th>\n",
       "      <th>Defensive</th>\n",
       "      <th>Infrastructure</th>\n",
       "      <th>Real Asset</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Digital Health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>5074022368.0</td>\n",
       "      <td>938972</td>\n",
       "      <td>US74319R1014</td>\n",
       "      <td>5074022368</td>\n",
       "      <td>...</td>\n",
       "      <td>46101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4295903310.0</td>\n",
       "      <td>923401</td>\n",
       "      <td>US0147521092</td>\n",
       "      <td>4295903310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46146</td>\n",
       "      <td>46027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>5000583878.0</td>\n",
       "      <td>68595V</td>\n",
       "      <td>US74164M1080</td>\n",
       "      <td>5000583878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>4295903261.0</td>\n",
       "      <td>933185</td>\n",
       "      <td>US0010551028</td>\n",
       "      <td>4295903261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>4295903341.0</td>\n",
       "      <td>916305</td>\n",
       "      <td>US0268747849</td>\n",
       "      <td>4295903341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Snapshot Date  Snapshot Date (Year) Weight Date  Rebal Date     Ex Date   \n",
       "0    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20  \\\n",
       "1    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "2    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "3    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "4    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "\n",
       "   SN3k index weight          OPID    DSCD          ISIN    Final ID  ...   \n",
       "0           0.000039  5074022368.0  938972  US74319R1014  5074022368  ...  \\\n",
       "1           0.000011  4295903310.0  923401  US0147521092  4295903310  ...   \n",
       "2           0.000167  5000583878.0  68595V  US74164M1080  5000583878  ...   \n",
       "3           0.000914  4295903261.0  933185  US0010551028  4295903261  ...   \n",
       "4           0.000946  4295903341.0  916305  US0268747849  4295903341  ...   \n",
       "\n",
       "  E-Commerce Battery Bio Revolution Clean Energy SaaS Defensive   \n",
       "0      46101     0.0            0.0          0.0  0.0       0.0  \\\n",
       "1        0.0     0.0            0.0          0.0  0.0       0.0   \n",
       "2        0.0     0.0            0.0          0.0  0.0       0.0   \n",
       "3        0.0     0.0            0.0          0.0  0.0       0.0   \n",
       "4        0.0     0.0            0.0          0.0  0.0       0.0   \n",
       "\n",
       "  Infrastructure Real Asset Inflation Digital Health  \n",
       "0            0.0        0.0     46027            0.0  \n",
       "1            0.0      46146     46027            0.0  \n",
       "2            0.0        0.0       0.0            0.0  \n",
       "3            0.0        0.0       0.0            0.0  \n",
       "4            0.0        0.0       0.0            0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2975, 23)\n",
      "(1643, 23)\n"
     ]
    }
   ],
   "source": [
    "def amendments(amend_df, input_df, max_year):\n",
    "    for index, row in amend_df.iterrows():\n",
    "        last_yr = row['last year']\n",
    "        index_name = row['Taxonomy']\n",
    "        isin = row['Identifier']\n",
    "        if last_yr == 'nan':\n",
    "            input_df.loc[input_df['ISIN'] == isin, index_name] = 0\n",
    "        else:\n",
    "            year_ls = list(range(int(float(last_yr) + 1), max_year))\n",
    "            input_df.loc[(input_df['Snapshot Date (Year)'].isin(year_ls)) & (input_df['ISIN'] == isin), index_name] = 0\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "\n",
    "\"\"\"Final Cleaning\"\"\"\n",
    "saas_isin = saas[saas['EK SaaS Check'] != 'Y']['isin'].unique()\n",
    "## df is the exposure or sector code table\n",
    "amend_df['last year'] = amend_df['last year'].astype(str)\n",
    "year = amend_df['last year'].unique().tolist()\n",
    "max_year = level1_exp_copy['Snapshot Date (Year)'].max()\n",
    "level1_exp_copy.loc[level1_exp_copy['ISIN'].isin(saas_isin), 'SaaS'] = 0\n",
    "output = amendments(amend_df, level1_exp_copy, max_year)\n",
    "\n",
    "display(output.head())\n",
    "output.fillna(0, inplace=True)\n",
    "output2 = output.loc[~(output[index_abv_list]==0).all(axis=1)]\n",
    "print(output.shape)\n",
    "print(output2.shape)\n",
    "\n",
    "#output2.to_excel('C:\\\\Users\\\\rzhou\\\\Downloads\\\\March13_Thematic_Data\\\\exp_level1_March13.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f83cfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.to_excel('group_level23_June13.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f154ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "['Syntax Thematic Cybersecurity Index']\n",
      "[44240 44246 44243]\n",
      "{'44240': 'Cybersecurity Software', '44246': 'Cybersecurity Services', '44243': 'Cybersecurity Hardware'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(rbr_id_map)\n\u001b[0;32m     24\u001b[0m rbr_data \u001b[39m=\u001b[39m get_rbr_data(df2, rbr_id)\n\u001b[1;32m---> 26\u001b[0m max_weight_code \u001b[39m=\u001b[39m get_max_weight_code(rbr_data, rbr_id_map)\n\u001b[0;32m     28\u001b[0m df_new \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_fix, max_weight_code], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m exp_code \u001b[39m=\u001b[39m exp_or_code(choice)\n",
      "Cell \u001b[1;32mIn[9], line 117\u001b[0m, in \u001b[0;36mget_max_weight_code\u001b[1;34m(get_rbr_data_op, rbr_id_map)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"This method aims to obtain the column with max weight code.\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m max_weight \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(get_rbr_data_op\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mMax Weight\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 117\u001b[0m max_weight_code \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(get_rbr_data_op\u001b[39m.\u001b[39;49midxmax(axis\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m'\u001b[39;49m), columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mMax Weighted Sector Code\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    118\u001b[0m new_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([get_rbr_data_op, max_weight_code, max_weight], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    119\u001b[0m new_df[\u001b[39m'\u001b[39m\u001b[39mMax Weighted Sector Name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m new_df[\u001b[39m'\u001b[39m\u001b[39mMax Weighted Sector Code\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(rbr_id_map)\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:10635\u001b[0m, in \u001b[0;36mDataFrame.idxmax\u001b[1;34m(self, axis, skipna, numeric_only)\u001b[0m\n\u001b[0;32m  10632\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m  10633\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m> 10635\u001b[0m res \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49m_reduce(\n\u001b[0;32m  10636\u001b[0m     nanops\u001b[39m.\u001b[39;49mnanargmax, \u001b[39m\"\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m\"\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, numeric_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m  10637\u001b[0m )\n\u001b[0;32m  10638\u001b[0m indices \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39m_values\n\u001b[0;32m  10640\u001b[0m \u001b[39m# indices will always be np.ndarray since axis is not None and\u001b[39;00m\n\u001b[0;32m  10641\u001b[0m \u001b[39m# values is a 2d array for DataFrame\u001b[39;00m\n\u001b[0;32m  10642\u001b[0m \u001b[39m# error: Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:10504\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10499\u001b[0m \u001b[39melif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m  10500\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(df\u001b[39m.\u001b[39mindex) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m  10501\u001b[0m         \u001b[39m# Taking a transpose would result in no columns, losing the dtype.\u001b[39;00m\n\u001b[0;32m  10502\u001b[0m         \u001b[39m# In the empty case, reducing along axis 0 or 1 gives the same\u001b[39;00m\n\u001b[0;32m  10503\u001b[0m         \u001b[39m# result dtype, so reduce with axis=0 and ignore values\u001b[39;00m\n\u001b[1;32m> 10504\u001b[0m         result \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49m_reduce(\n\u001b[0;32m  10505\u001b[0m             op,\n\u001b[0;32m  10506\u001b[0m             name,\n\u001b[0;32m  10507\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m  10508\u001b[0m             skipna\u001b[39m=\u001b[39;49mskipna,\n\u001b[0;32m  10509\u001b[0m             numeric_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m  10510\u001b[0m             filter_type\u001b[39m=\u001b[39;49mfilter_type,\n\u001b[0;32m  10511\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[0;32m  10512\u001b[0m         )\u001b[39m.\u001b[39miloc[:\u001b[39m0\u001b[39m]\n\u001b[0;32m  10513\u001b[0m         result\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mindex\n\u001b[0;32m  10514\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:10519\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10515\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mT\n\u001b[0;32m  10517\u001b[0m \u001b[39m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  10518\u001b[0m \u001b[39m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 10519\u001b[0m res \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mreduce(blk_func)\n\u001b[0;32m  10520\u001b[0m out \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_constructor(res)\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n\u001b[0;32m  10521\u001b[0m \u001b[39mif\u001b[39;00m out_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1534\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1532\u001b[0m res_blocks: \u001b[39mlist\u001b[39m[Block] \u001b[39m=\u001b[39m []\n\u001b[0;32m   1533\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m-> 1534\u001b[0m     nbs \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mreduce(func)\n\u001b[0;32m   1535\u001b[0m     res_blocks\u001b[39m.\u001b[39mextend(nbs)\n\u001b[0;32m   1537\u001b[0m index \u001b[39m=\u001b[39m Index([\u001b[39mNone\u001b[39;00m])  \u001b[39m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:339\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce\u001b[39m(\u001b[39mself\u001b[39m, func) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Block]:\n\u001b[0;32m    335\u001b[0m     \u001b[39m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[39m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m--> 339\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m    341\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    342\u001b[0m         \u001b[39m# TODO(EA2D): special case not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    343\u001b[0m         res_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[result]])\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:10482\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m  10480\u001b[0m     \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39m_reduce(name, skipna\u001b[39m=\u001b[39mskipna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m  10481\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m> 10482\u001b[0m     \u001b[39mreturn\u001b[39;00m op(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\nanops.py:96\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(invalid\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 96\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m is_object_dtype(args[\u001b[39m0\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\reaz_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\nanops.py:1147\u001b[0m, in \u001b[0;36mnanargmax\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m   1145\u001b[0m values, mask, _, _, _ \u001b[39m=\u001b[39m _get_values(values, \u001b[39mTrue\u001b[39;00m, fill_value_typ\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m, mask\u001b[39m=\u001b[39mmask)\n\u001b[0;32m   1146\u001b[0m \u001b[39m# error: Need type annotation for 'result'\u001b[39;00m\n\u001b[1;32m-> 1147\u001b[0m result \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39margmax(axis)  \u001b[39m# type: ignore[var-annotated]\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m result \u001b[39m=\u001b[39m _maybe_arg_null_out(result, axis, mask, skipna)\n\u001b[0;32m   1149\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "### Test ###\n",
    "\n",
    "\n",
    "df2=df2[df2['fiscal_year'].isna()]\n",
    "output = df5.copy()\n",
    "level_1 = False\n",
    "choice = 1\n",
    "df5_key = df5[['Snapshot Date (Year)', 'Final ID']]\n",
    "    \n",
    "for i in range(13):\n",
    "    index_name = index_list[i]\n",
    "    tax_l = [index_name]\n",
    "    if level_1:\n",
    "        level_l = [1]\n",
    "    else:\n",
    "        level_l = df3[df3['Taxonomy Name Matched to Map'] == index_name]['Taxonomy Level Used'].values.tolist()\n",
    "        print(level_l)\n",
    "    print(tax_l)\n",
    "    rbr_id = get_rbr_id(df1, tax_l, level_l)\n",
    "    print(rbr_id)\n",
    "    rbr_id_map = get_rbr_id_map(df1, tax_l, level_l)\n",
    "    print(rbr_id_map)\n",
    "\n",
    "    rbr_data = get_rbr_data(df2, rbr_id)\n",
    "\n",
    "    max_weight_code = get_max_weight_code(rbr_data, rbr_id_map)\n",
    "        \n",
    "    df_new = pd.concat([df_fix, max_weight_code], axis=1)\n",
    "        \n",
    "    exp_code = exp_or_code(choice)\n",
    "        \n",
    "    sub_df = df_new[df_new[exp_code].notna()][['Perm ID', 'fiscal_year', exp_code]] # extract non-NaN\n",
    "    row_labels = get_row_label(df_new)\n",
    "    exp_table = create_exp_table(df_new, row_labels)\n",
    "        \n",
    "    exp_filled = fill_exposure(df_new, exp_table, exp_code) # fill exposure or code table\n",
    "    exp_filled_fb = front_back_fill(exp_filled) # front and back fill\n",
    "    exp_timeseries = create_timeseries(exp_filled_fb)\n",
    "    \n",
    "    print(exp_filled_fb)\n",
    "    \n",
    "    print(exp_timeseries)\n",
    "\n",
    "    opid_list = exp_timeseries.columns[1:]\n",
    "    col_data = get_index_weight(df5_key, exp_timeseries)\n",
    "        #print(col_data[:10])\n",
    "    col_name = index_abv_list[i]\n",
    "    output.loc[:, col_name] = col_data # add a new column with column name equal to the index name\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
