{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d943d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique Perm ID: 11157\n",
      "['Syntax Thematic Cybersecurity Index', 'Syntax Thematic Cloud Compute Index', 'Syntax Thematic IoT Index', 'Syntax Thematic E-commerce Index', 'Syntax Thematic Battery Index', 'Syntax Thematic Bio Revolution Index', 'Syntax Thematic Clean Energy Index', 'Syntax Thematic Software-as-a-Service (SaaS) Index', 'Syntax Thematic Defensive Index', 'Syntax Thematic Infrastructure Index', 'Syntax Thematic Real Asset Index', 'Syntax Thematic Inflation Index', 'Syntax Thematic Digital Health Index']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import static_pulls as s_pulls \n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "\"\"\"import \"static_pulls\" and then call the method \n",
    "\"static_pulls.static_request(#today's date#, #list of unique ISINs from column AC#, ['OPID'], \n",
    "datatype = #placeholder#, write_df = False)\"\n",
    "setting write_df to false returns a pandas dataframe to the variable rather than writing a csv file to a folder\n",
    "\"\"\"\n",
    "\n",
    "def amendments(amend_df, input_df, max_year):\n",
    "    for index, row in amend_df.iterrows():\n",
    "        last_yr = row['last year']\n",
    "        index_name = row['Taxonomy']\n",
    "        isin = row['Identifier']\n",
    "        if last_yr == 'nan':\n",
    "            input_df.loc[input_df['ISIN'] == isin, index_name] = 0\n",
    "        else:\n",
    "            year_ls = list(range(int(float(last_yr) + 2), max_year + 1))\n",
    "            input_df.loc[(input_df['Snapshot Date (Year)'].isin(year_ls)) & (input_df['ISIN'] == isin), index_name] = 0\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def create_exp_table(df_new, row_labels):\n",
    "    \"\"\"Create the empty time series table\"\"\"\n",
    "    unique_perid = sorted(df_new['Perm ID'].unique())\n",
    "    columns = ['Row Labels'] + unique_perid\n",
    "    exp_table = pd.DataFrame(columns=columns)\n",
    "    exp_table['Row Labels'] = pd.Series(row_labels)\n",
    "    exp_table.set_index('Row Labels', inplace=True)   \n",
    "    return exp_table\n",
    "\n",
    "def create_timeseries(df):\n",
    "    \"\"\"Create the final time series by removing the last row and adding Year Used column\"\"\"\n",
    "    df_cleaned = df.iloc[:df.shape[0] - 1, :]\n",
    "    year_used = df_cleaned.index.astype(int) + 1\n",
    "    year_used = year_used.tolist()\n",
    "    df_op = df_cleaned.copy()\n",
    "    df_op.insert(loc=0, column='Year Used', value=year_used)\n",
    "    return df_op\n",
    "\n",
    "def exp_or_code(choice):\n",
    "    \"\"\"This method allows users to choose between max exposure or its sector code\"\"\"\n",
    "    if choice == 1:\n",
    "        return 'Max Weight'\n",
    "    else:\n",
    "        return 'Max Weighted Sector Code'\n",
    "\n",
    "def fill_NA_opid(isin_opid_map):\n",
    "    \"\"\"This method aims to fill NaN value of Perm ID by its corresponding ISIN\"\"\"\n",
    "    temp = isin_opid_map.copy()\n",
    "    for key, val in temp.items():\n",
    "        if val == 'NA':\n",
    "            temp[key] = key\n",
    "    return temp\n",
    "    \n",
    "def fill_exposure(df_tax_1, PG_bio_revo, exp_code):\n",
    "    \"\"\"Fill max exposure or the sector code into the time series table\"\"\"\n",
    "    for index, row in df_tax_1.iterrows():\n",
    "        year = row['fiscal_year']\n",
    "        perid = row['Perm ID']\n",
    "        PG_bio_revo.loc[year, perid] = row[exp_code]\n",
    "    return PG_bio_revo\n",
    "\n",
    "def front_back_fill(df):\n",
    "    \"\"\"Front fill and back fill the time series of each thematic index\"\"\"\n",
    "    output = df.ffill(axis=0)\n",
    "    output.fillna(method='bfill', inplace=True)\n",
    "    output.fillna(0, inplace=True)\n",
    "    return output\n",
    "\n",
    "def get_rbr_id(df_rbr, taxonomy_ls, level_ls):\n",
    "    \"\"\"This method aims to get the rbr id of the selected taxonomy and levels.\n",
    "    The input include a list of desired taxonomy and a list of desired levels.\n",
    "    taxonomy_ls is a list of strings, and level_ls is a list of integers.\"\"\"\n",
    "    sub_df = df_rbr.loc[(df_rbr.taxonomy.isin(taxonomy_ls)) & df_rbr.level.isin(level_ls)]\n",
    "    return sub_df.rbr_id.values\n",
    "\n",
    "def get_rbr_id_map(df_rbr, taxonomy_ls, level_ls):\n",
    "    \"\"\"Obtain the dictionary of key as rbr id and value as rbr name\"\"\"\n",
    "    sub_df = df_rbr.loc[(df_rbr.taxonomy.isin(taxonomy_ls)) & df_rbr.level.isin(level_ls)]\n",
    "    rbr_id_map = dict(zip(sub_df.rbr_id.apply(str), sub_df.name))\n",
    "    return rbr_id_map\n",
    "\n",
    "def get_rbr_data(df_rbr_data, rbr_id):\n",
    "    \"\"\"This method aims to get columns correspondent to the selected rbr id, and then clean \n",
    "    the data by taking the absolute value and replace 0 by 100000.\n",
    "    The rbr_id is a numpy array of integers. The df_rbr_data is the rbr data package.\"\"\"\n",
    "    output = []\n",
    "    for x in rbr_id:\n",
    "        col = str(x)\n",
    "        if col in df2.columns.tolist():\n",
    "            output.append(df2[col])\n",
    "        else:\n",
    "            print(col, ' -- ID Not Found in rbr data')\n",
    "    new_df = pd.concat(output, axis=1).abs()\n",
    "    new_df.replace(0, 100000, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def get_index_weight(df5_key, df_timeseries):\n",
    "    \"\"\"This method aims to obtain the columns of the time series table\"\"\"\n",
    "    output = [np.nan] * len(df5_key)\n",
    "    opid_list = df_timeseries.columns[1:]\n",
    "    for index, row in df5_key.iterrows():\n",
    "        year = row['Snapshot Date (Year)']\n",
    "        opid = str(row['Final ID'])\n",
    "        if opid in opid_list:\n",
    "            val = df_timeseries[df_timeseries['Year Used'] == year].loc[:,opid].values[0]\n",
    "            output[index] = val\n",
    "    return output\n",
    "\n",
    "def get_max_weight_code(get_rbr_data_op, rbr_id_map):\n",
    "    \"\"\"This method aims to obtain the column with max weight code.\"\"\"\n",
    "    max_weight = pd.DataFrame(get_rbr_data_op.max(axis=1), columns=['Max Weight'])\n",
    "    max_weight_code = pd.DataFrame(get_rbr_data_op.idxmax(axis='columns'), columns=['Max Weighted Sector Code'])\n",
    "    new_df = pd.concat([get_rbr_data_op, max_weight_code, max_weight], axis=1)\n",
    "    new_df['Max Weighted Sector Name'] = new_df['Max Weighted Sector Code'].map(rbr_id_map)\n",
    "    return new_df\n",
    "\n",
    "def get_row_label(df_tax_1):\n",
    "    \"\"\"Obtain the index of the time series table\"\"\"\n",
    "    row_labels = sorted(df_tax_1[df_tax_1['fiscal_year'].notna()]['fiscal_year'].unique().astype(int))\n",
    "    row_labels += [np.nan]\n",
    "    return row_labels\n",
    "\n",
    "def max_weight_code_name(index_list, level_1, df_fix, df1, df2, df3, df5, choice):\n",
    "    \"\"\"Obtain the dataframe ready for screening and weighting\"\"\"\n",
    "    # level_1 is a boolean variable which determines whether we focus on level 1 or the level with highest exp\n",
    "    output = df5.copy()\n",
    "    df5_key = df5[['Snapshot Date (Year)', 'Final ID']]\n",
    "    \n",
    "    for i in range(len(index_list)):\n",
    "        index_name = index_list[i]\n",
    "        tax_l = [index_name]\n",
    "        if level_1:\n",
    "            level_l = [1] #####\n",
    "        else:\n",
    "            level_l = df3[df3['Taxonomy Name Matched to Map'] == index_name]['Taxonomy Level Used'].values.tolist()\n",
    "            print(level_l)\n",
    "        print(tax_l)\n",
    "        rbr_id = get_rbr_id(df1, tax_l, level_l)\n",
    "        print(rbr_id)\n",
    "        rbr_id_map = get_rbr_id_map(df1, tax_l, level_l)\n",
    "        print(rbr_id_map)\n",
    "        \n",
    "        ############\n",
    "        \"\"\"\n",
    "        if index_name == 'Syntax Thematic Digital Health Index':\n",
    "            rbr_id = rbr_id[rbr_id != 46059]\n",
    "            rbr_id_map.pop(str(46059))\n",
    "            \n",
    "            print(rbr_id)\n",
    "            print(rbr_id_map)\"\"\"\n",
    "        ############\n",
    "        \n",
    "        rbr_data = get_rbr_data(df2, rbr_id)\n",
    "\n",
    "        max_weight_code = get_max_weight_code(rbr_data, rbr_id_map)\n",
    "        \n",
    "        df_new = pd.concat([df_fix, max_weight_code], axis=1)\n",
    "        \n",
    "        exp_code = exp_or_code(choice)\n",
    "        \n",
    "        sub_df = df_new[df_new[exp_code].notna()][['Perm ID', 'fiscal_year', exp_code]] # extract non-NaN\n",
    "        row_labels = get_row_label(df_new)\n",
    "        exp_table = create_exp_table(df_new, row_labels)\n",
    "        \n",
    "        exp_filled = fill_exposure(df_new, exp_table, exp_code) # fill exposure or code table\n",
    "        exp_filled_fb = front_back_fill(exp_filled) # front and back fill\n",
    "        exp_timeseries = create_timeseries(exp_filled_fb)\n",
    "\n",
    "        opid_list = exp_timeseries.columns[1:]\n",
    "        col_data = get_index_weight(df5_key, exp_timeseries)\n",
    "        #print(col_data[:10])\n",
    "        col_name = index_abv_list[i]\n",
    "        output.loc[:, col_name] = col_data # add a new column with column name equal to the index name\n",
    "        \n",
    "    return output   \n",
    "\n",
    "# change gvkey column from number to string\n",
    "df2 = pd.read_excel('no_filter_rbr_data_package_2023-06-13_v3.xlsx', \n",
    "                    converters={'gvkey': str})\n",
    "df1 = pd.read_excel('rbr_id_map_2023-06-12_no_filter.xlsx')\n",
    "df1 = df1.loc[:, ~df1.columns.str.contains('^Unnamed')]\n",
    "df3 = pd.read_excel('taxonomy_name_level4.xlsx')\n",
    "df5 = pd.read_excel('thematic_initial7_snapshot.xlsx')\n",
    "amend_df = pd.read_excel('classification_amend.xlsx')\n",
    "saas = pd.read_excel('SaaS.xlsx')\n",
    "\n",
    "df5['OPID'] = df5['OPID'].astype(str)\n",
    "df5['Final ID'] = df5['Final ID'].astype(str)\n",
    "df5['DSCD'] = df5['DSCD'].astype(str)\n",
    "df5['ISIN'] = df5['ISIN'].astype(str)\n",
    "\n",
    "# Front Fill gvkey due to the excel format\n",
    "df2['gvkey'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Front Fill fiscal_year after groupby gvkey\n",
    "# df2.update(df2.groupby('gvkey')['fiscal_year'].apply(lambda x: x.ffill()))\n",
    "\n",
    "# Sort df2 by gvkey and fiscal_date\n",
    "df2 = df2.sort_values(['gvkey', 'fiscal_date'])\n",
    "\n",
    "# Drop duplicates of gvkey and fiscal_year and keep the last one\n",
    "df2.drop_duplicates(subset=['gvkey', 'fiscal_year'], keep='last', inplace=True)\n",
    "df2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# fill NAN in fiscal_year by latest year\n",
    "df2['fiscal_year'].fillna(2022, inplace=True)\n",
    "\n",
    "# create df_fix by extracting the first 4 columns\n",
    "df_fix = df2.iloc[:, :4]\n",
    "\n",
    "# Fill the empty ISIN by its corresponding gvkey\n",
    "df_fix['isin'].fillna(df_fix['gvkey'], inplace=True)\n",
    "\n",
    "# Add Perm ID based on ISIN\n",
    "ticks = df_fix['isin'].unique()\n",
    "asofdate = dt.date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Run static_request to pull OPID\n",
    "\"\"\"USE mappings from thematic_initial\"\"\"\n",
    "#get_stats = s_pulls.static_request(asofdate, ticks, ['OPID'], 'placeholder', write_df = False)\n",
    "#isin_opid_map = dict(zip(get_stats.Instrument, get_stats.Value))\n",
    "isin_opid_map = dict(zip(df5['ISIN'], df5['Final ID']))\n",
    "\n",
    "# Fill NaN Perm ID\n",
    "isin_opid_map2 = fill_NA_opid(isin_opid_map)\n",
    "df_fix.insert(0, 'Perm ID', df_fix['isin'].map(isin_opid_map2))\n",
    "df_fix['Perm ID'].fillna(df_fix['isin'], inplace=True)\n",
    "\n",
    "print('The number of unique Perm ID:', df_fix['Perm ID'].nunique())\n",
    "\n",
    "index_list = df3['Taxonomy Name Matched to Map'].values.tolist()\n",
    "print(index_list)\n",
    "\n",
    "index_abv_list = ['Cybersecurity', 'Cloud', 'IoT', 'E-Commerce', 'Battery', 'Bio Revolution', 'Clean Energy', \n",
    "                  'SaaS', 'Defensive', 'Infrastructure', 'Real Asset', 'Inflation','Digital Health']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dae6dd8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Syntax Thematic Cybersecurity Index']\n",
      "[44239]\n",
      "{'44239': 'Syntax Thematic Cybersecurity Index'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reaz_\\AppData\\Local\\Temp\\ipykernel_9868\\2735201883.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5_key['Snapshot Date (Year)'] = df5_key['Snapshot Date (Year)'].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Syntax Thematic Cloud Compute Index']\n",
      "[46080]\n",
      "{'46080': 'Syntax Thematic Cloud Compute Index'}\n",
      "['Syntax Thematic IoT Index']\n",
      "[46087]\n",
      "{'46087': 'Syntax Thematic IoT Index'}\n",
      "['Syntax Thematic E-commerce Index']\n",
      "[46098]\n",
      "{'46098': 'Syntax Thematic E-commerce Index'}\n",
      "['Syntax Thematic Battery Index']\n",
      "[41753]\n",
      "{'41753': 'Syntax Thematic Battery Index'}\n",
      "['Syntax Thematic Bio Revolution Index']\n",
      "[44209]\n",
      "{'44209': 'Syntax Thematic Bio Revolution Index'}\n",
      "['Syntax Thematic Clean Energy Index']\n",
      "[44217]\n",
      "{'44217': 'Syntax Thematic Clean Energy Index'}\n",
      "['Syntax Thematic Software-as-a-Service (SaaS) Index']\n",
      "[46112]\n",
      "{'46112': 'Syntax Thematic Software-as-a-Service (SaaS) Index'}\n",
      "['Syntax Thematic Defensive Index']\n",
      "[46125]\n",
      "{'46125': 'Syntax Thematic Defensive Index'}\n",
      "['Syntax Thematic Infrastructure Index']\n",
      "[46130]\n",
      "{'46130': 'Syntax Thematic Infrastructure Index'}\n",
      "['Syntax Thematic Real Asset Index']\n",
      "[46144]\n",
      "{'46144': 'Syntax Thematic Real Asset Index'}\n",
      "['Syntax Thematic Inflation Index']\n",
      "[46017]\n",
      "{'46017': 'Syntax Thematic Inflation Index'}\n",
      "['Syntax Thematic Digital Health Index']\n",
      "[46051]\n",
      "{'46051': 'Syntax Thematic Digital Health Index'}\n",
      "50.08390498161316\n"
     ]
    }
   ],
   "source": [
    "def max_weight_code_name(index_list, level_1, df_fix, df1, df2, df3, df5, choice):\n",
    "    \"\"\"Obtain the dataframe ready for screening and weighting\"\"\"\n",
    "    # level_1 is a boolean variable which determines whether we focus on level 1 or the level with highest exp\n",
    "    output = df5.copy()\n",
    "    df5_key = df5[['Snapshot Date (Year)', 'Final ID']]\n",
    "    df5_key['Snapshot Date (Year)'] = df5_key['Snapshot Date (Year)'].astype(int)\n",
    "    \n",
    "    for i in range(len(index_list)):\n",
    "        index_name = index_list[i]\n",
    "        tax_l = [index_name]\n",
    "        if level_1:\n",
    "            level_l = [1]\n",
    "        else:\n",
    "            level_l = df3[df3['Taxonomy Name Matched to Map'] == index_name]['Taxonomy Level Used'].values.tolist()\n",
    "            print(level_l)\n",
    "        print(tax_l)\n",
    "        rbr_id = get_rbr_id(df1, tax_l, level_l)\n",
    "        print(rbr_id)\n",
    "        rbr_id_map = get_rbr_id_map(df1, tax_l, level_l)\n",
    "        print(rbr_id_map)\n",
    "        \n",
    "        ############\n",
    "        \"\"\"\n",
    "        if index_name == 'Syntax Thematic Digital Health Index':\n",
    "            rbr_id = rbr_id[rbr_id != 46059]\n",
    "            rbr_id_map.pop(str(46059))\n",
    "            \n",
    "            print(rbr_id)\n",
    "            print(rbr_id_map)\"\"\"\n",
    "        ############\n",
    "        \n",
    "        rbr_data = get_rbr_data(df2, rbr_id)\n",
    "\n",
    "        max_weight_code = get_max_weight_code(rbr_data, rbr_id_map)\n",
    "        \n",
    "        df_new = pd.concat([df_fix, max_weight_code], axis=1)\n",
    "        \n",
    "        exp_code = exp_or_code(choice)\n",
    "        \n",
    "        sub_df = df_new[df_new[exp_code].notna()][['Perm ID', 'fiscal_year', exp_code]] # extract non-NaN\n",
    "        row_labels = get_row_label(df_new)\n",
    "        exp_table = create_exp_table(df_new, row_labels)\n",
    "        \n",
    "        exp_filled = fill_exposure(df_new, exp_table, exp_code) # fill exposure or code table\n",
    "        exp_filled_fb = front_back_fill(exp_filled) # front and back fill\n",
    "        exp_timeseries = create_timeseries(exp_filled_fb)\n",
    "\n",
    "        opid_list = exp_timeseries.columns[1:]\n",
    "        col_data = get_index_weight(df5_key, exp_timeseries)\n",
    "        #print(col_data[:10])\n",
    "        col_name = index_abv_list[i]\n",
    "        output.loc[:, col_name] = col_data # add a new column with column name equal to the index name\n",
    "        \n",
    "    return output   \n",
    "\n",
    "start = time.time()\n",
    "# False means not limit to level 1\n",
    "# df_fix is the first 4 columns of rbr data with perm ID \n",
    "# df1 is the rbr map, df2 is the rbr data, and df5 is copied from the rbr selected calc (syntax 3000)\n",
    "#level23_weight = max_weight_code_name(index_list, False, df_fix, df1, df2, df5, 2)\n",
    "level1_weight = max_weight_code_name(index_list, True, df_fix, df1, df2, df3, df5, 1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93f9a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique Perm ID: 11157\n",
      "The number of unique Perm ID: 2997\n"
     ]
    }
   ],
   "source": [
    "print('The number of unique Perm ID:', df_fix['Perm ID'].nunique())\n",
    "print('The number of unique Perm ID:', df5['Final ID'].nunique())\n",
    "\n",
    "a = df_fix['isin'].unique().tolist()\n",
    "b = df5['ISIN'].unique().tolist()\n",
    "c = []\n",
    "for i in b:\n",
    "    if str(i) not in a:\n",
    "        c.append(i)\n",
    "        \n",
    "level1_exp = level1_weight[~level1_weight['ISIN'].isin(c)]\n",
    "level1_exp_copy = level1_exp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899eb1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Snapshot Date</th>\n",
       "      <th>Snapshot Date (Year)</th>\n",
       "      <th>Weight Date</th>\n",
       "      <th>Rebal Date</th>\n",
       "      <th>Ex Date</th>\n",
       "      <th>SN3k index weight</th>\n",
       "      <th>OPID</th>\n",
       "      <th>DSCD</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Final ID</th>\n",
       "      <th>...</th>\n",
       "      <th>E-Commerce</th>\n",
       "      <th>Battery</th>\n",
       "      <th>Bio Revolution</th>\n",
       "      <th>Clean Energy</th>\n",
       "      <th>SaaS</th>\n",
       "      <th>Defensive</th>\n",
       "      <th>Infrastructure</th>\n",
       "      <th>Real Asset</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Digital Health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>5074022368.0</td>\n",
       "      <td>938972</td>\n",
       "      <td>US74319R1014</td>\n",
       "      <td>5074022368</td>\n",
       "      <td>...</td>\n",
       "      <td>52.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183482</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4295903310.0</td>\n",
       "      <td>923401</td>\n",
       "      <td>US0147521092</td>\n",
       "      <td>4295903310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.829244</td>\n",
       "      <td>2.379333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>5000583878.0</td>\n",
       "      <td>68595V</td>\n",
       "      <td>US74164M1080</td>\n",
       "      <td>5000583878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>4295903261.0</td>\n",
       "      <td>933185</td>\n",
       "      <td>US0010551028</td>\n",
       "      <td>4295903261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>2023-06-20</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>4295903341.0</td>\n",
       "      <td>916305</td>\n",
       "      <td>US0268747849</td>\n",
       "      <td>4295903341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Snapshot Date  Snapshot Date (Year) Weight Date  Rebal Date     Ex Date   \n",
       "0    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20  \\\n",
       "1    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "2    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "3    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "4    2023-05-31                  2023  2023-06-07  2023-06-16  2023-06-20   \n",
       "\n",
       "   SN3k index weight          OPID    DSCD          ISIN    Final ID  ...   \n",
       "0           0.000039  5074022368.0  938972  US74319R1014  5074022368  ...  \\\n",
       "1           0.000011  4295903310.0  923401  US0147521092  4295903310  ...   \n",
       "2           0.000167  5000583878.0  68595V  US74164M1080  5000583878  ...   \n",
       "3           0.000914  4295903261.0  933185  US0010551028  4295903261  ...   \n",
       "4           0.000946  4295903341.0  916305  US0268747849  4295903341  ...   \n",
       "\n",
       "   E-Commerce  Battery  Bio Revolution  Clean Energy  SaaS  Defensive   \n",
       "0        52.2      0.0             0.0           0.0   0.0        0.0  \\\n",
       "1         0.0      0.0             0.0           0.0   0.0        0.0   \n",
       "2         0.0      0.0             0.0           0.0   0.0        0.0   \n",
       "3         0.0      0.0             0.0           0.0   0.0        0.0   \n",
       "4         0.0      0.0             0.0           0.0   0.0        0.0   \n",
       "\n",
       "   Infrastructure  Real Asset  Inflation  Digital Health  \n",
       "0             0.0    0.000000   2.183482             0.0  \n",
       "1             0.0   95.829244   2.379333             0.0  \n",
       "2             0.0    0.000000   0.000000             0.0  \n",
       "3             0.0    0.000000   0.000000             0.0  \n",
       "4             0.0    0.000000   0.000000             0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2975, 23)\n",
      "(1643, 23)\n"
     ]
    }
   ],
   "source": [
    "def amendments(amend_df, input_df, max_year):\n",
    "    for index, row in amend_df.iterrows():\n",
    "        last_yr = row['last year']\n",
    "        index_name = row['Taxonomy']\n",
    "        isin = row['Identifier']\n",
    "        if last_yr == 'nan':\n",
    "            input_df.loc[input_df['ISIN'] == isin, index_name] = 0\n",
    "        else:\n",
    "            year_ls = list(range(int(float(last_yr) + 1), max_year))\n",
    "            input_df.loc[(input_df['Snapshot Date (Year)'].isin(year_ls)) & (input_df['ISIN'] == isin), index_name] = 0\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "\n",
    "\"\"\"Final Cleaning\"\"\"\n",
    "saas_isin = saas[saas['EK SaaS Check'] != 'Y']['isin'].unique()\n",
    "## df is the exposure or sector code table\n",
    "amend_df['last year'] = amend_df['last year'].astype(str)\n",
    "year = amend_df['last year'].unique().tolist()\n",
    "max_year = level1_exp_copy['Snapshot Date (Year)'].max()\n",
    "level1_exp_copy.loc[level1_exp_copy['ISIN'].isin(saas_isin), 'SaaS'] = 0\n",
    "output = amendments(amend_df, level1_exp_copy, max_year)\n",
    "\n",
    "display(output.head())\n",
    "output.fillna(0, inplace=True)\n",
    "output2 = output.loc[~(output[index_abv_list]==0).all(axis=1)]\n",
    "print(output.shape)\n",
    "print(output2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e1e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.to_excel('exp_level1_March13.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eab7436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = pd.read_excel('C:\\\\Users\\\\rzhou\\\\Downloads\\\\March13_Thematic_Data\\\\exp_group_March13.xlsx')\n",
    "\n",
    "# yy = pd.read_excel('C:\\\\Users\\\\rzhou\\\\Downloads\\\\March13_Thematic_Data\\\\weight_date_data.xlsx')\n",
    "# yy.columns\n",
    "# xx['DSCD'] = xx['DSCD'].astype(str)\n",
    "# yy['DSCD'] = yy['DSCD'].astype(str)\n",
    "# xx2 = xx.merge(yy, on=['DSCD'], how='left')\n",
    "\n",
    "# display(xx2.head())\n",
    "\n",
    "# xx2.to_excel('C:\\\\Users\\\\rzhou\\\\Downloads\\\\March13_Thematic_Data\\\\exp_group_March13_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7255809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_excel('C:\\\\Users\\\\rzhou\\\\Downloads\\\\Project_1\\\\no_filter_rbr_data_package_2023-03-24_v3.xlsx', \n",
    "#                     converters={'gvkey': str})\n",
    "# df2.loc[df2['isin'] == 'US92537N1081', '46130']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
